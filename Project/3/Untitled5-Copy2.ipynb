{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avinashk94/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avinashk94/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import ndimage, misc\n",
    "import glob\n",
    "\n",
    "images = []\n",
    "i = 0   \n",
    "labels = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"proj3_images/Numerals/\"):\n",
    "    if dirnames!= []:\n",
    "        dirrr = dirnames\n",
    "    count = 0\n",
    "    for filename in filenames:\n",
    "        if \".png\" in filename:\n",
    "            count += 1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = ndimage.imread(filepath, mode=\"L\")\n",
    "            image_resized = misc.imresize(image, (28, 28))\n",
    "            images.append(image_resized)\n",
    "    if count != 0:\n",
    "        lMid = np.zeros((count,10))\n",
    "        lMid[:,int(dirrr[i])] = 1\n",
    "        if labels == []:\n",
    "            labels = lMid\n",
    "        else:\n",
    "            labels = np.vstack((labels,lMid))\n",
    "#         print(len(labels),len(labels[0]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uspsImages = np.asarray(images)\n",
    "uspsLabels = np.asarray(labels)\n",
    "uspsImages = uspsImages/255\n",
    "uspsImages = 1 - uspsImages\n",
    "uspsImages = uspsImages.reshape((-1,784))\n",
    "meanUSPSImg = np.mean(uspsImages,0)[:,np.newaxis]\n",
    "uspsImages = uspsImages.T\n",
    "uspsLabels = uspsLabels.T\n",
    "# print(meanUSPSImg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainImages = mnist.train.images.T\n",
    "testImages = mnist.test.images.T\n",
    "trainLabels = mnist.train.labels.T\n",
    "testLabels = mnist.test.labels.T\n",
    "validationImages = mnist.validation.images.T\n",
    "validationLabels = mnist.validation.labels.T\n",
    "m = trainImages.shape[1]\n",
    "\n",
    "batchSize = 100\n",
    "printEvery = 2\n",
    "nEpochs = 15\n",
    "nClasses = 10\n",
    "learningRate = 0.005 #IF used....\n",
    "lambdaa = 0.1 #Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [784,None])\n",
    "t = tf.placeholder(tf.float32, [10,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolve2D(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1] ,strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuralNetwork(x):\n",
    "    lambdaa = 0.1\n",
    "    nH1 = 512\n",
    "    weights = {'hiddenL1':tf.Variable(tf.random_normal([nH1,784])),\n",
    "               'hiddenL2':tf.Variable(tf.random_normal([10,nH1]))}\n",
    "    \n",
    "    biases = {'hiddenL1':tf.Variable(tf.random_normal([nH1,1])),\n",
    "              'hiddenL2':tf.Variable(tf.random_normal([10,1]))}\n",
    "    \n",
    "    w_h1 = tf.summary.histogram(\"weights1\",weights['hiddenL1'])\n",
    "    w_h2 = tf.summary.histogram(\"weights2\",weights['hiddenL2'])\n",
    "    b_h1 = tf.summary.histogram(\"biases1\",biases['hiddenL1'])\n",
    "    b_h2 = tf.summary.histogram(\"biases2\",biases['hiddenL2'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"a1\") as scope:\n",
    "        z1 = tf.matmul(weights['hiddenL1'],x) + biases['hiddenL1']\n",
    "        a1 = tf.nn.relu(z1)\n",
    "    \n",
    "    with tf.name_scope(\"a2\") as scope:\n",
    "        z2 = tf.matmul(weights['hiddenL2'],a1) + biases['hiddenL2']\n",
    "    \n",
    "    regLoss = tf.nn.l2_loss(weights['hiddenL1']) + tf.nn.l2_loss(weights['hiddenL1'])\n",
    "    return z2, regLoss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistiticRegression(x):\n",
    "    W = tf.Variable(tf.random_normal([10,784]))\n",
    "    b = tf.Variable(tf.zeros([10,1]))\n",
    "    \n",
    "    z = tf.matmul(W,x) + b\n",
    "    regLoss = tf.nn.l2_loss(W)\n",
    "    return z, regLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutionalNeuralNetwork(x):\n",
    "    weights = {'conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "               'conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "               'fullyC1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "               'fullyC2':tf.Variable(tf.random_normal([1024,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024,nClasses]))}\n",
    "    \n",
    "    biases = {'conv1':tf.Variable(tf.random_normal([32])),\n",
    "              'conv2':tf.Variable(tf.random_normal([64])),\n",
    "              'fullyC1':tf.Variable(tf.random_normal([1024])),\n",
    "              'fullyC2':tf.Variable(tf.random_normal([1024])),\n",
    "              'out':tf.Variable(tf.random_normal([nClasses]))}\n",
    "    \n",
    "    x = tf.transpose(x)\n",
    "    \n",
    "    Img = tf.reshape(x,[-1,28,28,1])\n",
    "    \n",
    "    conv1 = tf.nn.relu(convolve2D(Img,weights['conv1']) + biases['conv1'])\n",
    "    \n",
    "    conv1 = maxpool2d(conv1)\n",
    "    print(conv1)\n",
    "    \n",
    "    conv2 = tf.nn.relu(convolve2D(conv1,weights['conv2']) + biases['conv2'])\n",
    "#     print(conv2)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(conv2)\n",
    "    \n",
    "    conv2 = tf.reshape(conv2,[-1,7*7*64])\n",
    "#     print(conv2)\n",
    "    fcLayer1 = tf.nn.relu(tf.matmul(conv2,weights['fullyC1']) + biases['fullyC1'])\n",
    "#     print(fcLayer1)\n",
    "    fcLayer1 = tf.nn.dropout(fcLayer1,keepRate)\n",
    "    fcLayer2 = tf.nn.relu(tf.matmul(fcLayer1,weights['fullyC2']) + biases['fullyC2'])\n",
    "    output = tf.matmul(fcLayer2,weights['out']) + biases['out']\n",
    "    print(output)\n",
    "    \n",
    "    regLoss = tf.nn.l2_loss(weights['conv1']) + tf.nn.l2_loss(weights['conv2']) + tf.nn.l2_loss(weights['fullyC1']) + tf.nn.l2_loss(weights['fullyC1']) + tf.nn.l2_loss(weights['out'])\n",
    "    return tf.transpose(output), regLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolutionalNeuralNetwork2(x):\n",
    "    weights = {'conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "               'conv2':tf.Variable(tf.random_normal([5,5,32,32])),\n",
    "               'conv3':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "               'conv4':tf.Variable(tf.random_normal([5,5,64,64])),\n",
    "               'fullyC1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "               'fullyC2':tf.Variable(tf.random_normal([1024,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024,nClasses]))}\n",
    "    \n",
    "    biases = {'conv1':tf.Variable(tf.random_normal([32])),\n",
    "              'conv2':tf.Variable(tf.random_normal([32])),\n",
    "              'conv3':tf.Variable(tf.random_normal([64])),\n",
    "              'conv4':tf.Variable(tf.random_normal([64])),\n",
    "              'fullyC1':tf.Variable(tf.random_normal([1024])),\n",
    "              'fullyC2':tf.Variable(tf.random_normal([1024])),\n",
    "              'out':tf.Variable(tf.random_normal([nClasses]))}\n",
    "    \n",
    "    x = tf.transpose(x)\n",
    "    \n",
    "    Img = tf.reshape(x,[-1,28,28,1])\n",
    "    \n",
    "    conv1 = tf.nn.relu(convolve2D(Img,weights['conv1']) + biases['conv1'])\n",
    "    print(conv1)\n",
    "    conv2 = tf.nn.relu(convolve2D(conv1,weights['conv2']) + biases['conv2'])\n",
    "    print(conv2)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(conv2)\n",
    "    \n",
    "    conv3 = tf.nn.relu(convolve2D(conv2,weights['conv3']) + biases['conv3'])\n",
    "    print(conv3)\n",
    "    conv4 = tf.nn.relu(convolve2D(conv3,weights['conv4']) + biases['conv4'])\n",
    "    print(conv4)\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(conv4)\n",
    "    \n",
    "    conv4 = tf.reshape(conv4,[-1,7*7*64])\n",
    "    print(conv2)\n",
    "    fcLayer1 = tf.nn.relu(tf.matmul(conv4,weights['fullyC1']) + biases['fullyC1'])\n",
    "#     print(fcLayer1)\n",
    "    fcLayer1 = tf.nn.dropout(fcLayer1,keepRate)\n",
    "    fcLayer2 = tf.nn.relu(tf.matmul(fcLayer1,weights['fullyC2']) + biases['fullyC2'])\n",
    "    output = tf.matmul(fcLayer2,weights['out']) + biases['out']\n",
    "    print(output)\n",
    "    \n",
    "    return tf.transpose(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNetwork():\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(y),labels=tf.transpose(t)) + lambdaa*regLoss)\n",
    "        tf.summary.scalar(\"loss\",loss)\n",
    "    \n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct = tf.equal(tf.argmax(y),tf.argmax(t))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    mergeSummary = tf.summary.merge_all()\n",
    "\n",
    "    sess.run(init)\n",
    "    summaryWriter = tf.summary.FileWriter('../../TFout/2', sess.graph)\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "        error = 0.0\n",
    "        for i in range(int(m/batchSize)):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batchSize)\n",
    "            _, er, summaryStr = sess.run([optimizer,loss,mergeSummary],feed_dict={x:batch_xs.T,t:batch_ys.T})\n",
    "            summaryWriter.add_summary(summaryStr, epoch*(int(m/batchSize)) + i)\n",
    "            error += er\n",
    "        if (epoch+1)%printEvery == 0:\n",
    "            print('Loss in ',epoch+1,' epoch is ',error)\n",
    "\n",
    "    prediction = tf.equal(tf.argmax(y),tf.argmax(t))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction,\"float\"))\n",
    "    print(\"Accuracy Train:\", sess.run(accuracy,{x: trainImages, t: trainLabels}))\n",
    "    print(\"Accuracy validation:\", sess.run(accuracy,{x: validationImages, t: validationLabels}))\n",
    "    print(\"Accuracy Test:\", sess.run(accuracy,{x: testImages, t: testLabels}))\n",
    "    print(\"USPS Test Accuracy:\", sess.run(accuracy,{x: uspsImages, t: uspsLabels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in  5  epoch is  1337.58980238\n",
      "Loss in  10  epoch is  313.173126101\n",
      "Loss in  15  epoch is  304.621194184\n",
      "Loss in  20  epoch is  304.11099124\n",
      "Loss in  25  epoch is  304.133213043\n",
      "Loss in  30  epoch is  303.879255891\n",
      "Loss in  35  epoch is  303.834123164\n",
      "Loss in  40  epoch is  303.98941353\n",
      "Loss in  45  epoch is  303.990690917\n",
      "Loss in  50  epoch is  303.978442669\n",
      "Loss in  55  epoch is  303.925423294\n",
      "Loss in  60  epoch is  303.980923444\n",
      "Loss in  65  epoch is  303.890560091\n",
      "Loss in  70  epoch is  303.845578462\n",
      "Loss in  75  epoch is  303.860620052\n",
      "Loss in  80  epoch is  303.940401852\n",
      "Loss in  85  epoch is  303.954385012\n",
      "Loss in  90  epoch is  304.024800539\n",
      "Loss in  95  epoch is  304.038945079\n",
      "Loss in  100  epoch is  304.051430494\n",
      "Accuracy Train: 0.899018\n",
      "Accuracy validation: 0.9044\n",
      "Accuracy Test: 0.9045\n",
      "USPS Test Accuracy: 0.382269\n",
      "--- 152.29985189437866 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "y, regLoss = logistiticRegression(x)\n",
    "nEpochs = 100\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in  5  epoch is  80293.9880066\n",
      "Loss in  10  epoch is  906.527707696\n",
      "Loss in  15  epoch is  199.356616914\n",
      "Loss in  20  epoch is  185.630731091\n",
      "Loss in  25  epoch is  173.179935709\n",
      "Accuracy Train: 0.961691\n",
      "Accuracy validation: 0.9562\n",
      "Accuracy Test: 0.958\n",
      "USPS Test Accuracy: 0.473374\n",
      "--- 436.300900220871 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "y, regLoss = neuralNetwork(x)\n",
    "nEpochs = 25\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(?, 10), dtype=float32)\n",
      "Loss in  2  epoch is  25707417.5332\n",
      "Loss in  4  epoch is  19540667.6074\n",
      "Loss in  6  epoch is  17767107.7363\n",
      "Loss in  8  epoch is  16594658.3711\n",
      "Loss in  10  epoch is  15816090.8281\n",
      "Loss in  12  epoch is  15099346.666\n",
      "Loss in  14  epoch is  14346649.2969\n",
      "Accuracy Train: 0.990873\n",
      "Accuracy validation: 0.9786\n",
      "Accuracy Test: 0.9756\n",
      "USPS Test Accuracy: 0.582529\n",
      "--- 1576.4036478996277 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nEpochs = 14\n",
    "keepRate = 0.8\n",
    "start_time = time.time()\n",
    "y, regLoss = convolutionalNeuralNetwork(x)\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nEpochs = 2\n",
    "# keepRate = 0.7\n",
    "# start_time = time.time()\n",
    "# y = convolutionalNeuralNetwork2(x)\n",
    "# trainNetwork()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(?, 10), dtype=float32)\n",
      "Loss in  2  epoch is  164789167.344\n",
      "Loss in  4  epoch is  142050238.672\n",
      "Loss in  6  epoch is  125193693.078\n",
      "Loss in  8  epoch is  108960744.297\n",
      "Loss in  10  epoch is  92894150.6719\n",
      "Loss in  12  epoch is  77058803.0\n",
      "Loss in  14  epoch is  62208632.1094\n",
      "Accuracy Train: 0.991127\n",
      "Accuracy validation: 0.9802\n",
      "Accuracy Test: 0.9782\n",
      "USPS Test Accuracy: 0.611831\n",
      "--- 1533.0976750850677 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nEpochs = 14\n",
    "keepRate = 0.8\n",
    "start_time = time.time()\n",
    "y, regLoss = convolutionalNeuralNetwork(x)\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in  2  epoch is  1397.80503225\n",
      "Loss in  4  epoch is  609.988232017\n",
      "Loss in  6  epoch is  612.54486227\n",
      "Loss in  8  epoch is  613.822917163\n",
      "Loss in  10  epoch is  612.926954031\n",
      "Loss in  12  epoch is  615.499112189\n",
      "Loss in  14  epoch is  614.427107632\n",
      "Loss in  16  epoch is  614.569354713\n",
      "Loss in  18  epoch is  616.080063343\n",
      "Loss in  20  epoch is  614.699532568\n",
      "Loss in  22  epoch is  615.160694838\n",
      "Loss in  24  epoch is  615.188099205\n",
      "Loss in  26  epoch is  614.722010255\n",
      "Loss in  28  epoch is  615.799589157\n",
      "Loss in  30  epoch is  615.280618608\n",
      "Loss in  32  epoch is  615.502668619\n",
      "Loss in  34  epoch is  617.265633821\n",
      "Loss in  36  epoch is  615.239257574\n",
      "Loss in  38  epoch is  615.147463083\n",
      "Loss in  40  epoch is  616.363816142\n",
      "Loss in  42  epoch is  615.513518214\n",
      "Loss in  44  epoch is  616.159410715\n",
      "Loss in  46  epoch is  615.841395557\n",
      "Loss in  48  epoch is  614.631966531\n",
      "Loss in  50  epoch is  615.743518829\n",
      "Loss in  52  epoch is  616.380998909\n",
      "Loss in  54  epoch is  615.194377124\n",
      "Loss in  56  epoch is  615.141637802\n",
      "Loss in  58  epoch is  615.550329089\n",
      "Loss in  60  epoch is  615.56029737\n",
      "Loss in  62  epoch is  615.596296132\n",
      "Loss in  64  epoch is  615.337128937\n",
      "Loss in  66  epoch is  615.813102722\n",
      "Loss in  68  epoch is  615.671402156\n",
      "Loss in  70  epoch is  615.374402285\n",
      "Loss in  72  epoch is  615.528320611\n",
      "Loss in  74  epoch is  616.65822947\n",
      "Loss in  76  epoch is  615.440526605\n",
      "Loss in  78  epoch is  615.085366309\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.005\n",
    "start_time = time.time()\n",
    "y, regLoss = logistiticRegression(x)\n",
    "nEpochs = 100\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
