{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0.\n",
      "Use ``matplotlib.pyplot.imread`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scipy import ndimage, misc\n",
    "import glob\n",
    "\n",
    "images = []\n",
    "i = 0   \n",
    "labels = []\n",
    "\n",
    "for root, dirnames, filenames in os.walk(\"proj3_images/Numerals/\"):\n",
    "    if dirnames!= []:\n",
    "        dirrr = dirnames\n",
    "    count = 0\n",
    "    for filename in filenames:\n",
    "        if \".png\" in filename:\n",
    "            count += 1\n",
    "            filepath = os.path.join(root, filename)\n",
    "            image = ndimage.imread(filepath, mode=\"L\")\n",
    "            image_resized = misc.imresize(image, (28, 28))\n",
    "            images.append(image_resized)\n",
    "    if count != 0:\n",
    "        lMid = np.zeros((count,10))\n",
    "        lMid[:,int(dirrr[i])] = 1\n",
    "        if labels == []:\n",
    "            labels = lMid\n",
    "        else:\n",
    "            labels = np.vstack((labels,lMid))\n",
    "#         print(len(labels),len(labels[0]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "uspsImages = np.asarray(images)\n",
    "uspsLabels = np.asarray(labels)\n",
    "uspsImages = uspsImages/255\n",
    "uspsImages = 1 - uspsImages\n",
    "uspsImages = uspsImages.reshape((-1,784))\n",
    "meanUSPSImg = np.mean(uspsImages,0)[:,np.newaxis]\n",
    "uspsImages = uspsImages.T\n",
    "uspsLabels = uspsLabels.T\n",
    "# print(meanUSPSImg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = mnist.train.images.T\n",
    "testImages = mnist.test.images.T\n",
    "trainLabels = mnist.train.labels.T\n",
    "testLabels = mnist.test.labels.T\n",
    "validationImages = mnist.validation.images.T\n",
    "validationLabels = mnist.validation.labels.T\n",
    "m = trainImages.shape[1]\n",
    "\n",
    "batchSize = 100\n",
    "printEvery = 2\n",
    "nEpochs = 15\n",
    "nClasses = 10\n",
    "learningRate = 0.005 #IF used....\n",
    "lambdaa = 0.05 #Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [784,None])\n",
    "t = tf.placeholder(tf.float32, [10,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2D(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1] ,strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuralNetwork(x):\n",
    "    lambdaa = 0.1\n",
    "    nH1 = 512\n",
    "    weights = {'hiddenL1':tf.Variable(tf.random_normal([nH1,784])),\n",
    "               'hiddenL2':tf.Variable(tf.random_normal([10,nH1]))}\n",
    "    \n",
    "    biases = {'hiddenL1':tf.Variable(tf.random_normal([nH1,1])),\n",
    "              'hiddenL2':tf.Variable(tf.random_normal([10,1]))}\n",
    "    \n",
    "    w_h1 = tf.summary.histogram(\"weights1\",weights['hiddenL1'])\n",
    "    w_h2 = tf.summary.histogram(\"weights2\",weights['hiddenL2'])\n",
    "    b_h1 = tf.summary.histogram(\"biases1\",biases['hiddenL1'])\n",
    "    b_h2 = tf.summary.histogram(\"biases2\",biases['hiddenL2'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope(\"a1\") as scope:\n",
    "        z1 = tf.matmul(weights['hiddenL1'],x) + biases['hiddenL1']\n",
    "        a1 = tf.nn.relu(z1)\n",
    "    \n",
    "    with tf.name_scope(\"a2\") as scope:\n",
    "        z2 = tf.matmul(weights['hiddenL2'],a1) + biases['hiddenL2']\n",
    "    \n",
    "    regLoss = tf.nn.l2_loss(weights['hiddenL1']) + tf.nn.l2_loss(weights['hiddenL1'])\n",
    "    return z2, regLoss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistiticRegression(x):\n",
    "    W = tf.Variable(tf.random_normal([10,784]))\n",
    "    b = tf.Variable(tf.zeros([10,1]))\n",
    "    \n",
    "    z = tf.matmul(W,x) + b\n",
    "    regLoss = tf.nn.l2_loss(W)\n",
    "    return z, regLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutionalNeuralNetwork(x):\n",
    "    weights = {'conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "               'conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "               'fullyC1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "               'fullyC2':tf.Variable(tf.random_normal([1024,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024,nClasses]))}\n",
    "    \n",
    "    biases = {'conv1':tf.Variable(tf.random_normal([32])),\n",
    "              'conv2':tf.Variable(tf.random_normal([64])),\n",
    "              'fullyC1':tf.Variable(tf.random_normal([1024])),\n",
    "              'fullyC2':tf.Variable(tf.random_normal([1024])),\n",
    "              'out':tf.Variable(tf.random_normal([nClasses]))}\n",
    "    \n",
    "    x = tf.transpose(x)\n",
    "    \n",
    "    Img = tf.reshape(x,[-1,28,28,1])\n",
    "    \n",
    "    conv1 = tf.nn.relu(convolve2D(Img,weights['conv1']) + biases['conv1'])\n",
    "    \n",
    "    conv1 = maxpool2d(conv1)\n",
    "    print(conv1)\n",
    "    \n",
    "    conv2 = tf.nn.relu(convolve2D(conv1,weights['conv2']) + biases['conv2'])\n",
    "#     print(conv2)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(conv2)\n",
    "    \n",
    "    conv2 = tf.reshape(conv2,[-1,7*7*64])\n",
    "#     print(conv2)\n",
    "    fcLayer1 = tf.nn.relu(tf.matmul(conv2,weights['fullyC1']) + biases['fullyC1'])\n",
    "#     print(fcLayer1)\n",
    "    fcLayer1 = tf.nn.dropout(fcLayer1,keepRate)\n",
    "    fcLayer2 = tf.nn.relu(tf.matmul(fcLayer1,weights['fullyC2']) + biases['fullyC2'])\n",
    "    output = tf.matmul(fcLayer2,weights['out']) + biases['out']\n",
    "    print(output)\n",
    "    \n",
    "    regLoss = tf.nn.l2_loss(weights['conv1']) + tf.nn.l2_loss(weights['conv2']) + tf.nn.l2_loss(weights['fullyC1']) + tf.nn.l2_loss(weights['fullyC1']) + tf.nn.l2_loss(weights['out'])\n",
    "    return tf.transpose(output), regLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutionalNeuralNetwork2(x):\n",
    "    weights = {'conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "               'conv2':tf.Variable(tf.random_normal([5,5,32,32])),\n",
    "               'conv3':tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "               'conv4':tf.Variable(tf.random_normal([5,5,64,64])),\n",
    "               'fullyC1':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "               'fullyC2':tf.Variable(tf.random_normal([1024,1024])),\n",
    "               'out':tf.Variable(tf.random_normal([1024,nClasses]))}\n",
    "    \n",
    "    biases = {'conv1':tf.Variable(tf.random_normal([32])),\n",
    "              'conv2':tf.Variable(tf.random_normal([32])),\n",
    "              'conv3':tf.Variable(tf.random_normal([64])),\n",
    "              'conv4':tf.Variable(tf.random_normal([64])),\n",
    "              'fullyC1':tf.Variable(tf.random_normal([1024])),\n",
    "              'fullyC2':tf.Variable(tf.random_normal([1024])),\n",
    "              'out':tf.Variable(tf.random_normal([nClasses]))}\n",
    "    \n",
    "    x = tf.transpose(x)\n",
    "    \n",
    "    Img = tf.reshape(x,[-1,28,28,1])\n",
    "    \n",
    "    conv1 = tf.nn.relu(convolve2D(Img,weights['conv1']) + biases['conv1'])\n",
    "    print(conv1)\n",
    "    conv2 = tf.nn.relu(convolve2D(conv1,weights['conv2']) + biases['conv2'])\n",
    "    print(conv2)\n",
    "    conv2 = maxpool2d(conv2)\n",
    "    print(conv2)\n",
    "    \n",
    "    conv3 = tf.nn.relu(convolve2D(conv2,weights['conv3']) + biases['conv3'])\n",
    "    print(conv3)\n",
    "    conv4 = tf.nn.relu(convolve2D(conv3,weights['conv4']) + biases['conv4'])\n",
    "    print(conv4)\n",
    "    conv4 = maxpool2d(conv4)\n",
    "    print(conv4)\n",
    "    \n",
    "    conv4 = tf.reshape(conv4,[-1,7*7*64])\n",
    "    print(conv2)\n",
    "    fcLayer1 = tf.nn.relu(tf.matmul(conv4,weights['fullyC1']) + biases['fullyC1'])\n",
    "#     print(fcLayer1)\n",
    "    fcLayer1 = tf.nn.dropout(fcLayer1,keepRate)\n",
    "    fcLayer2 = tf.nn.relu(tf.matmul(fcLayer1,weights['fullyC2']) + biases['fullyC2'])\n",
    "    output = tf.matmul(fcLayer2,weights['out']) + biases['out']\n",
    "    print(output)\n",
    "    \n",
    "    return tf.transpose(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork():\n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=tf.transpose(y),labels=tf.transpose(t)) + lambdaa*regLoss)\n",
    "        tf.summary.scalar(\"loss\",loss)\n",
    "    \n",
    "    with tf.name_scope(\"training\") as scope:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(loss)\n",
    "    \n",
    "    with tf.name_scope(\"accuracy\") as scope:\n",
    "        correct = tf.equal(tf.argmax(y),tf.argmax(t))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
    "        \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "#     sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    mergeSummary = tf.summary.merge_all()\n",
    "\n",
    "    sess.run(init)\n",
    "    summaryWriter = tf.summary.FileWriter('../TFout/2', sess.graph)\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "        error = 0.0\n",
    "        for i in range(int(m/batchSize)):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batchSize)\n",
    "            _, er, summaryStr = sess.run([optimizer,loss,mergeSummary],feed_dict={x:batch_xs.T,t:batch_ys.T})\n",
    "            summaryWriter.add_summary(summaryStr, epoch*(int(m/batchSize)) + i)\n",
    "            error += er\n",
    "        if (epoch+1)%printEvery == 0:\n",
    "            print('Loss in ',epoch+1,' epoch is ',error)\n",
    "\n",
    "    prediction = tf.equal(tf.argmax(y),tf.argmax(t))\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction,\"float\"))\n",
    "    print(\"Accuracy Train:\", sess.run(accuracy,{x: trainImages, t: trainLabels}))\n",
    "    print(\"Accuracy validation:\", sess.run(accuracy,{x: validationImages, t: validationLabels}))\n",
    "    print(\"Accuracy Test:\", sess.run(accuracy,{x: testImages, t: testLabels}))\n",
    "    print(\"USPS Test Accuracy:\", sess.run(accuracy,{x: uspsImages, t: uspsLabels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in  2  epoch is  884.408898056\n",
      "Loss in  4  epoch is  495.468651772\n",
      "Loss in  6  epoch is  497.25446552\n",
      "Loss in  8  epoch is  498.18632102\n",
      "Loss in  10  epoch is  498.912643611\n",
      "Loss in  12  epoch is  500.266532779\n",
      "Loss in  14  epoch is  498.42451793\n",
      "Loss in  16  epoch is  499.89419806\n",
      "Loss in  18  epoch is  499.583696127\n",
      "Loss in  20  epoch is  499.894575655\n",
      "Loss in  22  epoch is  499.41582197\n",
      "Loss in  24  epoch is  499.156417608\n",
      "Loss in  26  epoch is  498.465442955\n",
      "Loss in  28  epoch is  499.220539153\n",
      "Loss in  30  epoch is  498.206918418\n",
      "Loss in  32  epoch is  499.455691516\n",
      "Loss in  34  epoch is  499.669592857\n",
      "Loss in  36  epoch is  499.538636684\n",
      "Loss in  38  epoch is  498.687748492\n",
      "Loss in  40  epoch is  498.349219263\n",
      "Loss in  42  epoch is  499.757096171\n",
      "Loss in  44  epoch is  499.75618434\n",
      "Loss in  46  epoch is  499.57224685\n",
      "Loss in  48  epoch is  499.71603924\n",
      "Loss in  50  epoch is  499.857765615\n",
      "Accuracy Train: 0.872418\n",
      "Accuracy validation: 0.88\n",
      "Accuracy Test: 0.8795\n",
      "USPS Test Accuracy: 0.375069\n",
      "--- 62.38675880432129 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "y, regLoss = logistiticRegression(x)\n",
    "nEpochs = 50\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in  2  epoch is  40911.297173\n",
      "Loss in  4  epoch is  673.150258541\n",
      "Loss in  6  epoch is  534.13668862\n",
      "Loss in  8  epoch is  437.399065644\n",
      "Loss in  10  epoch is  396.296700776\n",
      "Loss in  12  epoch is  379.704490364\n",
      "Loss in  14  epoch is  371.972845376\n",
      "Loss in  16  epoch is  342.853826374\n",
      "Loss in  18  epoch is  340.886722326\n",
      "Loss in  20  epoch is  324.201888233\n",
      "Loss in  22  epoch is  328.73327136\n",
      "Loss in  24  epoch is  315.390686363\n",
      "Accuracy Train: 0.898036\n",
      "Accuracy validation: 0.9084\n",
      "Accuracy Test: 0.9037\n",
      "USPS Test Accuracy: 0.39567\n",
      "--- 251.72521829605103 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "y, regLoss = neuralNetwork(x)\n",
    "nEpochs = 25\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"add_6:0\", shape=(?, 10), dtype=float32)\n",
      "Loss in  2  epoch is  66230774.8047\n",
      "Loss in  4  epoch is  49514400.8047\n",
      "Loss in  6  epoch is  36256153.0625\n",
      "Loss in  8  epoch is  24976231.5117\n",
      "Loss in  10  epoch is  15826835.168\n",
      "Loss in  12  epoch is  9046135.29395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nEpochs = 14\n",
    "keepRate = 0.8\n",
    "start_time = time.time()\n",
    "y, regLoss = convolutionalNeuralNetwork(x)\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nEpochs = 2\n",
    "# keepRate = 0.7\n",
    "# start_time = time.time()\n",
    "# y = convolutionalNeuralNetwork2(x)\n",
    "# trainNetwork()\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "Tensor(\"add_4:0\", shape=(?, 10), dtype=float32)\n",
      "Loss in  2  epoch is  164789167.344\n",
      "Loss in  4  epoch is  142050238.672\n",
      "Loss in  6  epoch is  125193693.078\n",
      "Loss in  8  epoch is  108960744.297\n",
      "Loss in  10  epoch is  92894150.6719\n",
      "Loss in  12  epoch is  77058803.0\n",
      "Loss in  14  epoch is  62208632.1094\n",
      "Accuracy Train: 0.991127\n",
      "Accuracy validation: 0.9802\n",
      "Accuracy Test: 0.9782\n",
      "USPS Test Accuracy: 0.611831\n",
      "--- 1533.0976750850677 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nEpochs = 14\n",
    "keepRate = 0.8\n",
    "start_time = time.time()\n",
    "y, regLoss = convolutionalNeuralNetwork(x)\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in  2  epoch is  1397.80503225\n",
      "Loss in  4  epoch is  609.988232017\n",
      "Loss in  6  epoch is  612.54486227\n",
      "Loss in  8  epoch is  613.822917163\n",
      "Loss in  10  epoch is  612.926954031\n",
      "Loss in  12  epoch is  615.499112189\n",
      "Loss in  14  epoch is  614.427107632\n",
      "Loss in  16  epoch is  614.569354713\n",
      "Loss in  18  epoch is  616.080063343\n",
      "Loss in  20  epoch is  614.699532568\n",
      "Loss in  22  epoch is  615.160694838\n",
      "Loss in  24  epoch is  615.188099205\n",
      "Loss in  26  epoch is  614.722010255\n",
      "Loss in  28  epoch is  615.799589157\n",
      "Loss in  30  epoch is  615.280618608\n",
      "Loss in  32  epoch is  615.502668619\n",
      "Loss in  34  epoch is  617.265633821\n",
      "Loss in  36  epoch is  615.239257574\n",
      "Loss in  38  epoch is  615.147463083\n",
      "Loss in  40  epoch is  616.363816142\n",
      "Loss in  42  epoch is  615.513518214\n",
      "Loss in  44  epoch is  616.159410715\n",
      "Loss in  46  epoch is  615.841395557\n",
      "Loss in  48  epoch is  614.631966531\n",
      "Loss in  50  epoch is  615.743518829\n",
      "Loss in  52  epoch is  616.380998909\n",
      "Loss in  54  epoch is  615.194377124\n",
      "Loss in  56  epoch is  615.141637802\n",
      "Loss in  58  epoch is  615.550329089\n",
      "Loss in  60  epoch is  615.56029737\n",
      "Loss in  62  epoch is  615.596296132\n",
      "Loss in  64  epoch is  615.337128937\n",
      "Loss in  66  epoch is  615.813102722\n",
      "Loss in  68  epoch is  615.671402156\n",
      "Loss in  70  epoch is  615.374402285\n",
      "Loss in  72  epoch is  615.528320611\n",
      "Loss in  74  epoch is  616.65822947\n",
      "Loss in  76  epoch is  615.440526605\n",
      "Loss in  78  epoch is  615.085366309\n",
      "Loss in  80  epoch is  614.919186294\n",
      "Loss in  82  epoch is  616.116334796\n",
      "Loss in  84  epoch is  616.266060114\n",
      "Loss in  86  epoch is  616.4240188\n",
      "Loss in  88  epoch is  615.702104628\n",
      "Loss in  90  epoch is  616.210050106\n",
      "Loss in  92  epoch is  616.562994599\n",
      "Loss in  94  epoch is  616.464858592\n",
      "Loss in  96  epoch is  615.671950936\n",
      "Loss in  98  epoch is  614.707444131\n",
      "Loss in  100  epoch is  614.999552369\n",
      "Accuracy Train: 0.848382\n",
      "Accuracy validation: 0.858\n",
      "Accuracy Test: 0.8585\n",
      "USPS Test Accuracy: 0.348767\n",
      "--- 3064.368362903595 seconds ---\n"
     ]
    }
   ],
   "source": [
    "learningRate = 0.005\n",
    "start_time = time.time()\n",
    "y, regLoss = logistiticRegression(x)\n",
    "nEpochs = 100\n",
    "trainNetwork()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
